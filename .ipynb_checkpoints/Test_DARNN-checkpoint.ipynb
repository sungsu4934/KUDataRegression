{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b50a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import main_regression as mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba8e2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a24b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 6. DARNN model (w/o data representation)\n",
    "config6 = {\n",
    "        'model': 'DARNN', # Regression에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC'} 중 택 1\n",
    "        'training': True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "        'best_model_path': './ckpt/darnn.pt',  # 학습 완료 모델 저장 경로\n",
    "        'parameter': {\n",
    "            'input_size': 27,  # 데이터의 변수 개수, int\n",
    "            'encoder_hidden_size': 64, # Encoder hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "            'decoder_hidden_size': 64, # Decoder hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "            'timestep': 32, # timestep의 크기, int(default: 16, 범위: 1이상)\n",
    "            'encoder_stateful': False, # Encoder의 Stateful 사용여부, bool(default: False)\n",
    "            'decoder_stateful': False, # Decoder의 Stateful 사용여부, bool(default: False)\n",
    "            'num_epochs': 3000,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "            'batch_size': 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "            'lr': 0.0001,  # learning rate, float(default: 0.001, 범위: 0.1 이하)\n",
    "            'device': 'cuda',  # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'need_yhist': True\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007ceeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13866, 27)\n",
      "(13866,)\n",
      "(5869, 27)\n",
      "(5869,)\n"
     ]
    }
   ],
   "source": [
    "# raw time seires data for regression\n",
    "train = pd.read_csv('./data/train_data.csv')\n",
    "test = pd.read_csv('./data/test_data.csv')\n",
    "\n",
    "train = train.drop('date', axis=1)\n",
    "test = test.drop('date', axis=1)\n",
    "\n",
    "train_x = train.drop('Appliances', axis = 1)\n",
    "train_y = train['Appliances']\n",
    "\n",
    "test_x = test.drop('Appliances', axis = 1)\n",
    "test_y = test['Appliances']\n",
    "\n",
    "train_data = {'x': train_x, 'y': train_y}\n",
    "test_data = {'x': test_x, 'y': test_y}\n",
    "\n",
    "print(train_x.shape)  #shape : (num_of_instance x representation_dims) = (7352, 64)\n",
    "print(train_y.shape) #shape : (num_of_instance) = (7352, )\n",
    "print(test_x.shape)  #shape : (num_of_instance x representation_dims) = (2947, 64)\n",
    "print(test_y.shape)  #shape : (num_of_instance) = (2947, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b679c1e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model\n",
      "\n",
      "Epoch 1/3000\n",
      "train Loss: 20699.0328\n",
      "val Loss: 17934.2401\n",
      "\n",
      "Epoch 10/3000\n",
      "train Loss: 7297.3219\n",
      "val Loss: 6316.9985\n",
      "\n",
      "Epoch 20/3000\n",
      "train Loss: 5097.5630\n",
      "val Loss: 4657.2414\n",
      "\n",
      "Epoch 30/3000\n",
      "train Loss: 4675.1732\n",
      "val Loss: 4294.1273\n",
      "\n",
      "Epoch 40/3000\n",
      "train Loss: 4552.4489\n",
      "val Loss: 4122.9331\n",
      "\n",
      "Epoch 50/3000\n",
      "train Loss: 4484.5556\n",
      "val Loss: 4096.4734\n",
      "\n",
      "Epoch 60/3000\n",
      "train Loss: 4405.1996\n",
      "val Loss: 4075.4865\n",
      "\n",
      "Epoch 70/3000\n",
      "train Loss: 4351.2986\n",
      "val Loss: 4058.7199\n",
      "\n",
      "Epoch 80/3000\n",
      "train Loss: 4289.5713\n",
      "val Loss: 4004.7064\n",
      "\n",
      "Epoch 90/3000\n",
      "train Loss: 4202.1096\n",
      "val Loss: 4017.2743\n",
      "\n",
      "Epoch 100/3000\n",
      "train Loss: 4107.0696\n",
      "val Loss: 4044.7086\n",
      "\n",
      "Epoch 110/3000\n",
      "train Loss: 4011.6777\n",
      "val Loss: 3968.1140\n",
      "\n",
      "Epoch 120/3000\n",
      "train Loss: 3861.8477\n",
      "val Loss: 4029.3304\n",
      "\n",
      "Epoch 130/3000\n",
      "train Loss: 3739.7790\n",
      "val Loss: 4090.3273\n",
      "\n",
      "Epoch 140/3000\n",
      "train Loss: 3582.9525\n",
      "val Loss: 4058.6658\n",
      "\n",
      "Epoch 150/3000\n",
      "train Loss: 3477.1969\n",
      "val Loss: 4117.7520\n",
      "\n",
      "Epoch 160/3000\n",
      "train Loss: 3327.8500\n",
      "val Loss: 4108.4052\n",
      "\n",
      "Epoch 170/3000\n",
      "train Loss: 3218.6991\n",
      "val Loss: 4248.0999\n",
      "\n",
      "Epoch 180/3000\n",
      "train Loss: 2979.1217\n",
      "val Loss: 4195.1464\n",
      "\n",
      "Epoch 190/3000\n",
      "train Loss: 2866.0164\n",
      "val Loss: 4429.4267\n",
      "\n",
      "Epoch 200/3000\n",
      "train Loss: 2938.8273\n",
      "val Loss: 4378.0894\n",
      "\n",
      "Epoch 210/3000\n",
      "train Loss: 2630.1670\n",
      "val Loss: 4637.5879\n",
      "\n",
      "Epoch 220/3000\n",
      "train Loss: 2512.8967\n",
      "val Loss: 4486.0526\n",
      "\n",
      "Epoch 230/3000\n",
      "train Loss: 2382.0527\n",
      "val Loss: 4566.7677\n",
      "\n",
      "Epoch 240/3000\n",
      "train Loss: 2400.9011\n",
      "val Loss: 4411.8681\n",
      "\n",
      "Epoch 250/3000\n",
      "train Loss: 2431.3212\n",
      "val Loss: 4568.7059\n",
      "\n",
      "Epoch 260/3000\n",
      "train Loss: 2332.5277\n",
      "val Loss: 4507.4576\n",
      "\n",
      "Epoch 270/3000\n",
      "train Loss: 2130.4044\n",
      "val Loss: 5068.7429\n",
      "\n",
      "Epoch 280/3000\n",
      "train Loss: 2030.6526\n",
      "val Loss: 4880.0461\n",
      "\n",
      "Epoch 290/3000\n",
      "train Loss: 1941.8560\n",
      "val Loss: 4750.6071\n",
      "\n",
      "Epoch 300/3000\n",
      "train Loss: 1861.4880\n",
      "val Loss: 4999.6420\n",
      "\n",
      "Epoch 310/3000\n",
      "train Loss: 1848.2130\n",
      "val Loss: 5076.5149\n",
      "\n",
      "Epoch 320/3000\n",
      "train Loss: 1732.8462\n",
      "val Loss: 5004.1068\n",
      "\n",
      "Epoch 330/3000\n",
      "train Loss: 1637.6526\n",
      "val Loss: 4878.0834\n",
      "\n",
      "Epoch 340/3000\n",
      "train Loss: 1556.2337\n",
      "val Loss: 5146.7174\n",
      "\n",
      "Epoch 350/3000\n",
      "train Loss: 1493.3010\n",
      "val Loss: 4975.4481\n",
      "\n",
      "Epoch 360/3000\n",
      "train Loss: 1625.6934\n",
      "val Loss: 5049.7997\n",
      "\n",
      "Epoch 370/3000\n",
      "train Loss: 1439.0545\n",
      "val Loss: 5165.5420\n",
      "\n",
      "Epoch 380/3000\n",
      "train Loss: 1377.8695\n",
      "val Loss: 5281.9756\n",
      "\n",
      "Epoch 390/3000\n",
      "train Loss: 1325.4798\n",
      "val Loss: 5318.2639\n",
      "\n",
      "Epoch 400/3000\n",
      "train Loss: 1294.5643\n",
      "val Loss: 5550.2158\n",
      "\n",
      "Epoch 410/3000\n",
      "train Loss: 1213.0282\n",
      "val Loss: 5513.9411\n",
      "\n",
      "Epoch 420/3000\n",
      "train Loss: 1270.5675\n",
      "val Loss: 5222.9654\n",
      "\n",
      "Epoch 430/3000\n",
      "train Loss: 1154.7466\n",
      "val Loss: 5640.2737\n",
      "\n",
      "Epoch 440/3000\n",
      "train Loss: 1104.5049\n",
      "val Loss: 5628.4935\n",
      "\n",
      "Epoch 450/3000\n",
      "train Loss: 1127.1171\n",
      "val Loss: 5632.5920\n",
      "\n",
      "Epoch 460/3000\n",
      "train Loss: 1259.4975\n",
      "val Loss: 5777.7393\n",
      "\n",
      "Epoch 470/3000\n",
      "train Loss: 996.3549\n",
      "val Loss: 5710.3294\n",
      "\n",
      "Epoch 480/3000\n",
      "train Loss: 1041.0533\n",
      "val Loss: 5961.9454\n",
      "\n",
      "Epoch 490/3000\n",
      "train Loss: 929.0516\n",
      "val Loss: 6204.8295\n",
      "\n",
      "Epoch 500/3000\n",
      "train Loss: 849.7657\n",
      "val Loss: 6001.5990\n",
      "\n",
      "Epoch 510/3000\n",
      "train Loss: 895.2442\n",
      "val Loss: 6035.4957\n",
      "\n",
      "Epoch 520/3000\n",
      "train Loss: 955.8180\n",
      "val Loss: 6329.3953\n",
      "\n",
      "Epoch 530/3000\n",
      "train Loss: 847.8419\n",
      "val Loss: 6205.2305\n",
      "\n",
      "Epoch 540/3000\n",
      "train Loss: 881.5095\n",
      "val Loss: 6014.1468\n",
      "\n",
      "Epoch 550/3000\n",
      "train Loss: 775.3059\n",
      "val Loss: 6229.6059\n",
      "\n",
      "Epoch 560/3000\n",
      "train Loss: 746.3507\n",
      "val Loss: 6235.7677\n",
      "\n",
      "Epoch 570/3000\n",
      "train Loss: 729.5349\n",
      "val Loss: 6364.5681\n",
      "\n",
      "Epoch 580/3000\n",
      "train Loss: 1133.5379\n",
      "val Loss: 6206.4255\n",
      "\n",
      "Epoch 590/3000\n",
      "train Loss: 686.3315\n",
      "val Loss: 6188.8016\n",
      "\n",
      "Epoch 600/3000\n",
      "train Loss: 683.8904\n",
      "val Loss: 6209.4558\n",
      "\n",
      "Epoch 610/3000\n",
      "train Loss: 837.0088\n",
      "val Loss: 6910.5859\n",
      "\n",
      "Epoch 620/3000\n",
      "train Loss: 584.6065\n",
      "val Loss: 6760.0290\n",
      "\n",
      "Epoch 630/3000\n",
      "train Loss: 981.0282\n",
      "val Loss: 6644.1254\n",
      "\n",
      "Epoch 640/3000\n",
      "train Loss: 608.5534\n",
      "val Loss: 6628.4919\n",
      "\n",
      "Epoch 650/3000\n",
      "train Loss: 639.9151\n",
      "val Loss: 6444.8682\n",
      "\n",
      "Epoch 660/3000\n",
      "train Loss: 566.3177\n",
      "val Loss: 6565.2013\n",
      "\n",
      "Epoch 670/3000\n",
      "train Loss: 1079.1022\n",
      "val Loss: 6852.5133\n",
      "\n",
      "Epoch 680/3000\n",
      "train Loss: 478.0476\n",
      "val Loss: 6473.8974\n",
      "\n",
      "Epoch 690/3000\n",
      "train Loss: 456.2465\n",
      "val Loss: 6389.4549\n",
      "\n",
      "Epoch 700/3000\n",
      "train Loss: 422.1662\n",
      "val Loss: 6519.8111\n",
      "\n",
      "Epoch 710/3000\n",
      "train Loss: 2944.8804\n",
      "val Loss: 4825.7448\n",
      "\n",
      "Epoch 720/3000\n",
      "train Loss: 3844.0386\n",
      "val Loss: 4234.0728\n",
      "\n",
      "Epoch 730/3000\n",
      "train Loss: 2975.6451\n",
      "val Loss: 4415.9405\n",
      "\n",
      "Epoch 740/3000\n",
      "train Loss: 2800.1031\n",
      "val Loss: 4538.1452\n",
      "\n",
      "Epoch 750/3000\n",
      "train Loss: 2373.5151\n",
      "val Loss: 4821.0848\n",
      "\n",
      "Epoch 760/3000\n",
      "train Loss: 2038.6791\n",
      "val Loss: 5314.0865\n",
      "\n",
      "Epoch 770/3000\n",
      "train Loss: 1844.9641\n",
      "val Loss: 5356.0150\n",
      "\n",
      "Epoch 780/3000\n",
      "train Loss: 1627.3049\n",
      "val Loss: 5634.4166\n",
      "\n",
      "Epoch 790/3000\n",
      "train Loss: 1460.2061\n",
      "val Loss: 5298.4228\n",
      "\n",
      "Epoch 800/3000\n",
      "train Loss: 1277.9314\n",
      "val Loss: 5704.7624\n",
      "\n",
      "Epoch 810/3000\n",
      "train Loss: 1423.6716\n",
      "val Loss: 6079.4959\n",
      "\n",
      "Epoch 820/3000\n",
      "train Loss: 1131.4900\n",
      "val Loss: 5739.6806\n",
      "\n",
      "Epoch 830/3000\n",
      "train Loss: 1094.3300\n",
      "val Loss: 5935.1152\n",
      "\n",
      "Epoch 840/3000\n",
      "train Loss: 1029.9719\n",
      "val Loss: 5805.3054\n",
      "\n",
      "Epoch 850/3000\n",
      "train Loss: 967.1693\n",
      "val Loss: 5680.8114\n",
      "\n",
      "Epoch 860/3000\n",
      "train Loss: 862.8979\n",
      "val Loss: 5660.5997\n",
      "\n",
      "Epoch 870/3000\n",
      "train Loss: 874.3923\n",
      "val Loss: 5696.6750\n",
      "\n",
      "Epoch 880/3000\n",
      "train Loss: 779.2900\n",
      "val Loss: 5904.5668\n",
      "\n",
      "Epoch 890/3000\n",
      "train Loss: 741.3246\n",
      "val Loss: 5920.6544\n",
      "\n",
      "Epoch 900/3000\n",
      "train Loss: 711.9601\n",
      "val Loss: 6006.3989\n",
      "\n",
      "Epoch 910/3000\n",
      "train Loss: 678.7921\n",
      "val Loss: 5581.2553\n",
      "\n",
      "Epoch 920/3000\n",
      "train Loss: 631.9306\n",
      "val Loss: 5994.7876\n",
      "\n",
      "Epoch 930/3000\n",
      "train Loss: 1179.2661\n",
      "val Loss: 5586.5166\n",
      "\n",
      "Epoch 940/3000\n",
      "train Loss: 588.7285\n",
      "val Loss: 5901.4152\n",
      "\n",
      "Epoch 950/3000\n",
      "train Loss: 3202.0859\n",
      "val Loss: 4844.0941\n",
      "\n",
      "Epoch 960/3000\n",
      "train Loss: 2022.8612\n",
      "val Loss: 4893.5486\n",
      "\n",
      "Epoch 970/3000\n",
      "train Loss: 2797.4387\n",
      "val Loss: 4394.2323\n",
      "\n",
      "Epoch 980/3000\n",
      "train Loss: 1607.0564\n",
      "val Loss: 5256.7799\n",
      "\n",
      "Epoch 990/3000\n",
      "train Loss: 1504.8392\n",
      "val Loss: 5571.8526\n",
      "\n",
      "Epoch 1000/3000\n",
      "train Loss: 1285.4516\n",
      "val Loss: 5741.0995\n",
      "\n",
      "Epoch 1010/3000\n",
      "train Loss: 1183.4847\n",
      "val Loss: 5908.5005\n",
      "\n",
      "Epoch 1020/3000\n",
      "train Loss: 1043.1290\n",
      "val Loss: 6065.5812\n",
      "\n",
      "Epoch 1030/3000\n",
      "train Loss: 988.9483\n",
      "val Loss: 5653.1106\n",
      "\n",
      "Epoch 1040/3000\n",
      "train Loss: 904.1998\n",
      "val Loss: 6218.8222\n",
      "\n",
      "Epoch 1050/3000\n",
      "train Loss: 897.3817\n",
      "val Loss: 6203.9919\n",
      "\n",
      "Epoch 1060/3000\n",
      "train Loss: 790.6818\n",
      "val Loss: 5959.5668\n",
      "\n",
      "Epoch 1070/3000\n",
      "train Loss: 754.2934\n",
      "val Loss: 5825.2233\n",
      "\n",
      "Epoch 1080/3000\n",
      "train Loss: 701.1896\n",
      "val Loss: 6104.5110\n",
      "\n",
      "Epoch 1090/3000\n",
      "train Loss: 660.9439\n",
      "val Loss: 6150.4484\n",
      "\n",
      "Epoch 1100/3000\n",
      "train Loss: 649.7001\n",
      "val Loss: 6026.3839\n",
      "\n",
      "Epoch 1110/3000\n",
      "train Loss: 636.3176\n",
      "val Loss: 6224.1234\n",
      "\n",
      "Epoch 1120/3000\n",
      "train Loss: 624.5294\n",
      "val Loss: 6454.0421\n",
      "\n",
      "Epoch 1130/3000\n",
      "train Loss: 565.6117\n",
      "val Loss: 6432.8022\n",
      "\n",
      "Epoch 1140/3000\n",
      "train Loss: 556.2849\n",
      "val Loss: 6502.1020\n",
      "\n",
      "Epoch 1150/3000\n",
      "train Loss: 548.6515\n",
      "val Loss: 6326.6505\n",
      "\n",
      "Epoch 1160/3000\n",
      "train Loss: 518.6599\n",
      "val Loss: 6514.1682\n",
      "\n",
      "Epoch 1170/3000\n",
      "train Loss: 533.2999\n",
      "val Loss: 6526.6410\n",
      "\n",
      "Epoch 1180/3000\n",
      "train Loss: 505.3489\n",
      "val Loss: 6355.6694\n",
      "\n",
      "Epoch 1190/3000\n",
      "train Loss: 489.6007\n",
      "val Loss: 6171.3515\n",
      "\n",
      "Epoch 1200/3000\n",
      "train Loss: 548.3551\n",
      "val Loss: 5899.9780\n",
      "\n",
      "Epoch 1210/3000\n",
      "train Loss: 476.0622\n",
      "val Loss: 6826.3251\n",
      "\n",
      "Epoch 1220/3000\n",
      "train Loss: 442.6185\n",
      "val Loss: 6468.7404\n",
      "\n",
      "Epoch 1230/3000\n",
      "train Loss: 466.6354\n",
      "val Loss: 6541.0896\n",
      "\n",
      "Epoch 1240/3000\n",
      "train Loss: 423.3828\n",
      "val Loss: 6529.9652\n",
      "\n",
      "Epoch 1250/3000\n",
      "train Loss: 385.5330\n",
      "val Loss: 6348.6840\n",
      "\n",
      "Epoch 1260/3000\n",
      "train Loss: 419.2615\n",
      "val Loss: 6388.7198\n",
      "\n",
      "Epoch 1270/3000\n",
      "train Loss: 419.6691\n",
      "val Loss: 6390.1874\n",
      "\n",
      "Epoch 1280/3000\n",
      "train Loss: 401.0159\n",
      "val Loss: 6604.0496\n",
      "\n",
      "Epoch 1290/3000\n",
      "train Loss: 368.1834\n",
      "val Loss: 6520.0687\n",
      "\n",
      "Epoch 1300/3000\n",
      "train Loss: 928.9269\n",
      "val Loss: 5588.0015\n",
      "\n",
      "Epoch 1310/3000\n",
      "train Loss: 479.6907\n",
      "val Loss: 6105.6096\n",
      "\n",
      "Epoch 1320/3000\n",
      "train Loss: 376.1097\n",
      "val Loss: 6396.7481\n",
      "\n",
      "Epoch 1330/3000\n",
      "train Loss: 358.6803\n",
      "val Loss: 6267.1156\n",
      "\n",
      "Epoch 1340/3000\n",
      "train Loss: 346.8905\n",
      "val Loss: 6353.1449\n",
      "\n",
      "Epoch 1350/3000\n",
      "train Loss: 386.3267\n",
      "val Loss: 6543.5595\n",
      "\n",
      "Epoch 1360/3000\n",
      "train Loss: 324.7951\n",
      "val Loss: 6470.0818\n",
      "\n",
      "Epoch 1370/3000\n",
      "train Loss: 338.3566\n",
      "val Loss: 6340.0685\n",
      "\n",
      "Epoch 1380/3000\n",
      "train Loss: 299.7186\n",
      "val Loss: 6402.3700\n",
      "\n",
      "Epoch 1390/3000\n",
      "train Loss: 322.8762\n",
      "val Loss: 6335.1914\n",
      "\n",
      "Epoch 1400/3000\n",
      "train Loss: 806.3393\n",
      "val Loss: 5785.9477\n",
      "\n",
      "Epoch 1410/3000\n",
      "train Loss: 2335.1623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 4796.5416\n",
      "\n",
      "Epoch 1420/3000\n",
      "train Loss: 1069.2262\n",
      "val Loss: 5396.7157\n",
      "\n",
      "Epoch 1430/3000\n",
      "train Loss: 672.1572\n",
      "val Loss: 5349.7548\n",
      "\n",
      "Epoch 1440/3000\n",
      "train Loss: 524.0819\n",
      "val Loss: 5637.2830\n",
      "\n",
      "Epoch 1450/3000\n",
      "train Loss: 481.7019\n",
      "val Loss: 5704.2955\n",
      "\n",
      "Epoch 1460/3000\n",
      "train Loss: 482.0773\n",
      "val Loss: 5939.6795\n",
      "\n",
      "Epoch 1470/3000\n",
      "train Loss: 1749.8272\n",
      "val Loss: 5475.6645\n",
      "\n",
      "Epoch 1480/3000\n",
      "train Loss: 767.6521\n",
      "val Loss: 5812.2316\n",
      "\n",
      "Epoch 1490/3000\n",
      "train Loss: 555.3886\n",
      "val Loss: 5751.0469\n",
      "\n",
      "Epoch 1500/3000\n",
      "train Loss: 528.2535\n",
      "val Loss: 5708.2273\n",
      "\n",
      "Epoch 1510/3000\n",
      "train Loss: 405.3732\n",
      "val Loss: 5880.0637\n",
      "\n",
      "Epoch 1520/3000\n",
      "train Loss: 365.7251\n",
      "val Loss: 5881.3050\n",
      "\n",
      "Epoch 1530/3000\n",
      "train Loss: 362.2319\n",
      "val Loss: 5665.8848\n",
      "\n",
      "Epoch 1540/3000\n",
      "train Loss: 359.2030\n",
      "val Loss: 5685.0128\n",
      "\n",
      "Epoch 1550/3000\n",
      "train Loss: 426.4297\n",
      "val Loss: 5740.4554\n",
      "\n",
      "Epoch 1560/3000\n",
      "train Loss: 320.1004\n",
      "val Loss: 5781.1630\n",
      "\n",
      "Epoch 1570/3000\n",
      "train Loss: 280.7802\n",
      "val Loss: 5901.3087\n",
      "\n",
      "Epoch 1580/3000\n",
      "train Loss: 271.3371\n",
      "val Loss: 5842.6624\n",
      "\n",
      "Epoch 1590/3000\n",
      "train Loss: 285.8694\n",
      "val Loss: 5868.5466\n",
      "\n",
      "Epoch 1600/3000\n",
      "train Loss: 251.8418\n",
      "val Loss: 5835.0194\n",
      "\n",
      "Epoch 1610/3000\n",
      "train Loss: 292.9931\n",
      "val Loss: 5744.7676\n",
      "\n",
      "Epoch 1620/3000\n",
      "train Loss: 256.7997\n",
      "val Loss: 5743.1868\n",
      "\n",
      "Epoch 1630/3000\n",
      "train Loss: 238.0207\n",
      "val Loss: 5860.3207\n",
      "\n",
      "Epoch 1640/3000\n",
      "train Loss: 231.8683\n",
      "val Loss: 5756.8859\n",
      "\n",
      "Epoch 1650/3000\n",
      "train Loss: 235.2484\n",
      "val Loss: 5773.4501\n",
      "\n",
      "Epoch 1660/3000\n",
      "train Loss: 238.7487\n",
      "val Loss: 5927.7508\n",
      "\n",
      "Epoch 1670/3000\n",
      "train Loss: 255.9319\n",
      "val Loss: 5751.9986\n",
      "\n",
      "Epoch 1680/3000\n",
      "train Loss: 224.2107\n",
      "val Loss: 5777.7083\n",
      "\n",
      "Epoch 1690/3000\n",
      "train Loss: 234.1223\n",
      "val Loss: 5745.5947\n",
      "\n",
      "Epoch 1700/3000\n",
      "train Loss: 215.2877\n",
      "val Loss: 5781.5110\n",
      "\n",
      "Epoch 1710/3000\n",
      "train Loss: 176.2406\n",
      "val Loss: 5779.4464\n",
      "\n",
      "Epoch 1720/3000\n",
      "train Loss: 204.9364\n",
      "val Loss: 5756.0272\n",
      "\n",
      "Epoch 1730/3000\n",
      "train Loss: 195.1132\n",
      "val Loss: 5674.2999\n",
      "\n",
      "Epoch 1740/3000\n",
      "train Loss: 215.7602\n",
      "val Loss: 5815.7864\n",
      "\n",
      "Epoch 1750/3000\n",
      "train Loss: 2044.8781\n",
      "val Loss: 5500.9256\n",
      "\n",
      "Epoch 1760/3000\n",
      "train Loss: 326.9685\n",
      "val Loss: 5907.1254\n",
      "\n",
      "Epoch 1770/3000\n",
      "train Loss: 239.7653\n",
      "val Loss: 5998.0790\n",
      "\n",
      "Epoch 1780/3000\n",
      "train Loss: 207.2190\n",
      "val Loss: 6067.6425\n",
      "\n",
      "Epoch 1790/3000\n",
      "train Loss: 222.7801\n",
      "val Loss: 6067.1673\n",
      "\n",
      "Epoch 1800/3000\n",
      "train Loss: 291.4013\n",
      "val Loss: 6042.5696\n",
      "\n",
      "Epoch 1810/3000\n",
      "train Loss: 343.8666\n",
      "val Loss: 6092.8196\n",
      "\n",
      "Epoch 1820/3000\n",
      "train Loss: 171.0574\n",
      "val Loss: 6030.2420\n",
      "\n",
      "Epoch 1830/3000\n",
      "train Loss: 195.1694\n",
      "val Loss: 5955.7675\n",
      "\n",
      "Epoch 1840/3000\n",
      "train Loss: 242.2105\n",
      "val Loss: 6298.8453\n",
      "\n",
      "Epoch 1850/3000\n",
      "train Loss: 329.3484\n",
      "val Loss: 5795.1798\n",
      "\n",
      "Epoch 1860/3000\n",
      "train Loss: 186.8821\n",
      "val Loss: 5916.0103\n",
      "\n",
      "Epoch 1870/3000\n",
      "train Loss: 230.4348\n",
      "val Loss: 5888.0180\n",
      "\n",
      "Epoch 1880/3000\n",
      "train Loss: 165.4779\n",
      "val Loss: 5912.9089\n",
      "\n",
      "Epoch 1890/3000\n",
      "train Loss: 160.3592\n",
      "val Loss: 5984.5380\n",
      "\n",
      "Epoch 1900/3000\n",
      "train Loss: 272.8106\n",
      "val Loss: 5976.0683\n",
      "\n",
      "Epoch 1910/3000\n",
      "train Loss: 159.8442\n",
      "val Loss: 5912.9028\n",
      "\n",
      "Epoch 1920/3000\n",
      "train Loss: 174.8607\n",
      "val Loss: 5900.6983\n",
      "\n",
      "Epoch 1930/3000\n",
      "train Loss: 169.0342\n",
      "val Loss: 5940.8906\n",
      "\n",
      "Epoch 1940/3000\n",
      "train Loss: 151.0829\n",
      "val Loss: 5842.4754\n",
      "\n",
      "Epoch 1950/3000\n",
      "train Loss: 157.9634\n",
      "val Loss: 5835.3069\n",
      "\n",
      "Epoch 1960/3000\n",
      "train Loss: 143.5426\n",
      "val Loss: 5827.1381\n",
      "\n",
      "Epoch 1970/3000\n",
      "train Loss: 286.2120\n",
      "val Loss: 6022.7115\n",
      "\n",
      "Epoch 1980/3000\n",
      "train Loss: 155.5294\n",
      "val Loss: 5900.5512\n",
      "\n",
      "Epoch 1990/3000\n",
      "train Loss: 120.0853\n",
      "val Loss: 5787.0537\n",
      "\n",
      "Epoch 2000/3000\n",
      "train Loss: 142.6116\n",
      "val Loss: 5761.9317\n",
      "\n",
      "Epoch 2010/3000\n",
      "train Loss: 123.2912\n",
      "val Loss: 5820.1326\n",
      "\n",
      "Epoch 2020/3000\n",
      "train Loss: 123.5034\n",
      "val Loss: 5786.4636\n",
      "\n",
      "Epoch 2030/3000\n",
      "train Loss: 294.4232\n",
      "val Loss: 5931.0647\n",
      "\n",
      "Epoch 2040/3000\n",
      "train Loss: 141.2273\n",
      "val Loss: 5780.7046\n",
      "\n",
      "Epoch 2050/3000\n",
      "train Loss: 130.4438\n",
      "val Loss: 5886.1199\n",
      "\n",
      "Epoch 2060/3000\n",
      "train Loss: 147.7834\n",
      "val Loss: 5877.1988\n",
      "\n",
      "Epoch 2070/3000\n",
      "train Loss: 158.9486\n",
      "val Loss: 5867.0789\n",
      "\n",
      "Epoch 2080/3000\n",
      "train Loss: 144.2128\n",
      "val Loss: 5755.1825\n",
      "\n",
      "Epoch 2090/3000\n",
      "train Loss: 129.8180\n",
      "val Loss: 5875.0360\n",
      "\n",
      "Epoch 2100/3000\n",
      "train Loss: 126.1814\n",
      "val Loss: 5785.7839\n",
      "\n",
      "Epoch 2110/3000\n",
      "train Loss: 119.5632\n",
      "val Loss: 5838.2099\n",
      "\n",
      "Epoch 2120/3000\n",
      "train Loss: 137.7150\n",
      "val Loss: 5826.8615\n",
      "\n",
      "Epoch 2130/3000\n",
      "train Loss: 136.3695\n",
      "val Loss: 5754.7280\n",
      "\n",
      "Epoch 2140/3000\n",
      "train Loss: 136.1545\n",
      "val Loss: 5741.1693\n",
      "\n",
      "Epoch 2150/3000\n",
      "train Loss: 113.9168\n",
      "val Loss: 5770.1614\n",
      "\n",
      "Epoch 2160/3000\n",
      "train Loss: 118.7218\n",
      "val Loss: 5903.9336\n",
      "\n",
      "Epoch 2170/3000\n",
      "train Loss: 132.6515\n",
      "val Loss: 5749.3021\n",
      "\n",
      "Epoch 2180/3000\n",
      "train Loss: 125.0574\n",
      "val Loss: 5760.6715\n",
      "\n",
      "Epoch 2190/3000\n",
      "train Loss: 108.6408\n",
      "val Loss: 5701.9378\n",
      "\n",
      "Epoch 2200/3000\n",
      "train Loss: 103.8756\n",
      "val Loss: 5852.3639\n",
      "\n",
      "Epoch 2210/3000\n",
      "train Loss: 102.4926\n",
      "val Loss: 5694.3414\n",
      "\n",
      "Epoch 2220/3000\n",
      "train Loss: 118.1654\n",
      "val Loss: 5740.2149\n",
      "\n",
      "Epoch 2230/3000\n",
      "train Loss: 96.2919\n",
      "val Loss: 5783.5999\n",
      "\n",
      "Epoch 2240/3000\n",
      "train Loss: 119.8227\n",
      "val Loss: 5750.9974\n",
      "\n",
      "Epoch 2250/3000\n",
      "train Loss: 105.4080\n",
      "val Loss: 5749.8855\n",
      "\n",
      "Epoch 2260/3000\n",
      "train Loss: 117.1929\n",
      "val Loss: 5827.4182\n",
      "\n",
      "Epoch 2270/3000\n",
      "train Loss: 97.4808\n",
      "val Loss: 5765.7909\n",
      "\n",
      "Epoch 2280/3000\n",
      "train Loss: 157.7378\n",
      "val Loss: 5753.1939\n",
      "\n",
      "Epoch 2290/3000\n",
      "train Loss: 171.1891\n",
      "val Loss: 5785.8059\n",
      "\n",
      "Epoch 2300/3000\n",
      "train Loss: 156.2668\n",
      "val Loss: 5798.8587\n",
      "\n",
      "Epoch 2310/3000\n",
      "train Loss: 120.8741\n",
      "val Loss: 5734.3138\n",
      "\n",
      "Epoch 2320/3000\n",
      "train Loss: 95.1468\n",
      "val Loss: 5696.5723\n",
      "\n",
      "Epoch 2330/3000\n",
      "train Loss: 97.7107\n",
      "val Loss: 5775.9092\n",
      "\n",
      "Epoch 2340/3000\n",
      "train Loss: 246.9859\n",
      "val Loss: 5711.7859\n",
      "\n",
      "Epoch 2350/3000\n",
      "train Loss: 85.6094\n",
      "val Loss: 5751.3341\n",
      "\n",
      "Epoch 2360/3000\n",
      "train Loss: 84.3973\n",
      "val Loss: 5751.9878\n",
      "\n",
      "Epoch 2370/3000\n",
      "train Loss: 119.9544\n",
      "val Loss: 5743.0588\n",
      "\n",
      "Epoch 2380/3000\n",
      "train Loss: 100.6516\n",
      "val Loss: 5753.2569\n",
      "\n",
      "Epoch 2390/3000\n",
      "train Loss: 103.7095\n",
      "val Loss: 5726.2774\n",
      "\n",
      "Epoch 2400/3000\n",
      "train Loss: 83.8661\n",
      "val Loss: 5733.7434\n",
      "\n",
      "Epoch 2410/3000\n",
      "train Loss: 96.5499\n",
      "val Loss: 5818.1331\n",
      "\n",
      "Epoch 2420/3000\n",
      "train Loss: 81.5727\n",
      "val Loss: 5742.9915\n",
      "\n",
      "Epoch 2430/3000\n",
      "train Loss: 83.0643\n",
      "val Loss: 5692.5318\n",
      "\n",
      "Epoch 2440/3000\n",
      "train Loss: 108.7935\n",
      "val Loss: 5761.3658\n",
      "\n",
      "Epoch 2450/3000\n",
      "train Loss: 85.9306\n",
      "val Loss: 5693.8521\n",
      "\n",
      "Epoch 2460/3000\n",
      "train Loss: 68.5710\n",
      "val Loss: 5746.4818\n",
      "\n",
      "Epoch 2470/3000\n",
      "train Loss: 66.6041\n",
      "val Loss: 5764.9235\n",
      "\n",
      "Epoch 2480/3000\n",
      "train Loss: 119.8450\n",
      "val Loss: 5718.2084\n",
      "\n",
      "Epoch 2490/3000\n",
      "train Loss: 105.7393\n",
      "val Loss: 5761.7551\n",
      "\n",
      "Epoch 2500/3000\n",
      "train Loss: 90.1228\n",
      "val Loss: 5753.6185\n",
      "\n",
      "Epoch 2510/3000\n",
      "train Loss: 83.3675\n",
      "val Loss: 5691.6155\n",
      "\n",
      "Epoch 2520/3000\n",
      "train Loss: 66.4598\n",
      "val Loss: 5723.9884\n",
      "\n",
      "Epoch 2530/3000\n",
      "train Loss: 93.3332\n",
      "val Loss: 5727.1583\n",
      "\n",
      "Epoch 2540/3000\n",
      "train Loss: 66.4363\n",
      "val Loss: 5719.6288\n",
      "\n",
      "Epoch 2550/3000\n",
      "train Loss: 94.1139\n",
      "val Loss: 5763.1748\n",
      "\n",
      "Epoch 2560/3000\n",
      "train Loss: 175.5908\n",
      "val Loss: 5750.5840\n",
      "\n",
      "Epoch 2570/3000\n",
      "train Loss: 94.0317\n",
      "val Loss: 5812.2675\n",
      "\n",
      "Epoch 2580/3000\n",
      "train Loss: 102.0708\n",
      "val Loss: 5707.9223\n",
      "\n",
      "Epoch 2590/3000\n",
      "train Loss: 66.2779\n",
      "val Loss: 5722.3559\n",
      "\n",
      "Epoch 2600/3000\n",
      "train Loss: 80.4746\n",
      "val Loss: 5800.5092\n",
      "\n",
      "Epoch 2610/3000\n",
      "train Loss: 97.3107\n",
      "val Loss: 5748.8717\n",
      "\n",
      "Epoch 2620/3000\n",
      "train Loss: 92.0928\n",
      "val Loss: 5824.1406\n",
      "\n",
      "Epoch 2630/3000\n",
      "train Loss: 69.7562\n",
      "val Loss: 5708.7546\n",
      "\n",
      "Epoch 2640/3000\n",
      "train Loss: 59.0128\n",
      "val Loss: 5707.8373\n",
      "\n",
      "Epoch 2650/3000\n",
      "train Loss: 99.4103\n",
      "val Loss: 5771.8416\n",
      "\n",
      "Epoch 2660/3000\n",
      "train Loss: 62.7479\n",
      "val Loss: 5760.8922\n",
      "\n",
      "Epoch 2670/3000\n",
      "train Loss: 75.4201\n",
      "val Loss: 5870.0620\n",
      "\n",
      "Epoch 2680/3000\n",
      "train Loss: 73.5510\n",
      "val Loss: 5742.3903\n",
      "\n",
      "Epoch 2690/3000\n",
      "train Loss: 78.1419\n",
      "val Loss: 5691.6367\n",
      "\n",
      "Epoch 2700/3000\n",
      "train Loss: 72.9011\n",
      "val Loss: 5719.8453\n",
      "\n",
      "Epoch 2710/3000\n",
      "train Loss: 99.0703\n",
      "val Loss: 5691.8428\n",
      "\n",
      "Epoch 2720/3000\n",
      "train Loss: 76.4709\n",
      "val Loss: 5771.5366\n",
      "\n",
      "Epoch 2730/3000\n",
      "train Loss: 56.3796\n",
      "val Loss: 5737.8856\n",
      "\n",
      "Epoch 2740/3000\n",
      "train Loss: 91.2234\n",
      "val Loss: 5727.6930\n",
      "\n",
      "Epoch 2750/3000\n",
      "train Loss: 77.6838\n",
      "val Loss: 5778.6062\n",
      "\n",
      "Epoch 2760/3000\n",
      "train Loss: 802.9928\n",
      "val Loss: 5709.8385\n",
      "\n",
      "Epoch 2770/3000\n",
      "train Loss: 223.7867\n",
      "val Loss: 5711.0146\n",
      "\n",
      "Epoch 2780/3000\n",
      "train Loss: 101.3833\n",
      "val Loss: 5815.7664\n",
      "\n",
      "Epoch 2790/3000\n",
      "train Loss: 86.1701\n",
      "val Loss: 5825.8376\n",
      "\n",
      "Epoch 2800/3000\n",
      "train Loss: 108.3514\n",
      "val Loss: 5771.7430\n",
      "\n",
      "Epoch 2810/3000\n",
      "train Loss: 74.3438\n",
      "val Loss: 5775.0725\n",
      "\n",
      "Epoch 2820/3000\n",
      "train Loss: 114.7045\n",
      "val Loss: 5836.4038\n",
      "\n",
      "Epoch 2830/3000\n",
      "train Loss: 79.8338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 5799.5667\n",
      "\n",
      "Epoch 2840/3000\n",
      "train Loss: 82.4437\n",
      "val Loss: 5819.5426\n",
      "\n",
      "Epoch 2850/3000\n",
      "train Loss: 83.4620\n",
      "val Loss: 5700.6845\n",
      "\n",
      "Epoch 2860/3000\n",
      "train Loss: 54.6590\n",
      "val Loss: 5761.7430\n",
      "\n",
      "Epoch 2870/3000\n",
      "train Loss: 68.0846\n",
      "val Loss: 5754.9534\n",
      "\n",
      "Epoch 2880/3000\n",
      "train Loss: 88.3055\n",
      "val Loss: 5787.7809\n",
      "\n",
      "Epoch 2890/3000\n",
      "train Loss: 65.4002\n",
      "val Loss: 5721.0143\n",
      "\n",
      "Epoch 2900/3000\n",
      "train Loss: 75.3017\n",
      "val Loss: 5681.1212\n",
      "\n",
      "Epoch 2910/3000\n",
      "train Loss: 186.7745\n",
      "val Loss: 5820.2508\n",
      "\n",
      "Epoch 2920/3000\n",
      "train Loss: 56.8239\n",
      "val Loss: 5787.4161\n",
      "\n",
      "Epoch 2930/3000\n",
      "train Loss: 57.9500\n",
      "val Loss: 5806.3182\n",
      "\n",
      "Epoch 2940/3000\n",
      "train Loss: 75.2876\n",
      "val Loss: 5768.8699\n",
      "\n",
      "Epoch 2950/3000\n",
      "train Loss: 59.1611\n",
      "val Loss: 5786.7017\n",
      "\n",
      "Epoch 2960/3000\n",
      "train Loss: 66.2749\n",
      "val Loss: 5703.1135\n",
      "\n",
      "Epoch 2970/3000\n",
      "train Loss: 62.2006\n",
      "val Loss: 5739.6133\n",
      "\n",
      "Epoch 2980/3000\n",
      "train Loss: 62.6038\n",
      "val Loss: 5721.5913\n",
      "\n",
      "Epoch 2990/3000\n",
      "train Loss: 138.7861\n",
      "val Loss: 5699.9409\n",
      "\n",
      "Epoch 3000/3000\n",
      "train Loss: 46.9438\n",
      "val Loss: 5722.2446\n",
      "\n",
      "Training complete in 667m 51s\n",
      "Best val Loss: 3966.150055\n",
      "\n",
      "Start testing data\n",
      "\n",
      "test Loss: 3544.4462890625\n"
     ]
    }
   ],
   "source": [
    "# Case 6. DARNN model (w/o data representation)\n",
    "config = config6\n",
    "data_reg = mr.Regression(config, train_data, test_data)\n",
    "model = data_reg.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_reg.train_model(model)  # 모델 학습\n",
    "    data_reg.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse = data_reg.pred_data(model, best_model_path=config[\"best_model_path\"])  # class 예측\n",
    "print(f'test Loss: {mse}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8c93687a9f3cac7ea1a38989caebc63561608f7a862e4f9a11f0ba4f68d9d9a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
