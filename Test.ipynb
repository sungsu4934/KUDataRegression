{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b50a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import main_regression as mr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ca4b9",
   "metadata": {},
   "source": [
    "# Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba8e2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 42\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9815b94",
   "metadata": {},
   "source": [
    "# Set Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a24b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1. LSTM model (w/o data representation)\n",
    "config1 = {\n",
    "        'model': 'LSTM', # Regression에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC', 'DARNN} 중 택 1\n",
    "        'training': True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "        'best_model_path': './ckpt/lstm.pt',  # 학습 완료 모델 저장 경로\n",
    "        'parameter': {\n",
    "            'input_size': 27,  # 데이터의 변수 개수, int\n",
    "            'timestep' : 10, # timestep = window_size\n",
    "            'shift_size': 1, # shift 정도, int\n",
    "            'num_classes': 1,  # 분류할 class 개수, int\n",
    "            'num_layers': 2,  # recurrent layers의 수, int(default: 2, 범위: 1 이상)\n",
    "            'hidden_size': 64,  # hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "            'dropout': 0.1,  # dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "            'bidirectional': True,  # 모델의 양방향성 여부, bool(default: True)\n",
    "            'num_epochs': 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "            'batch_size': 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "            'lr': 0.0001,  # learning rate, float(default: 0.001, 범위: 0.1 이하)\n",
    "            'device': 'cuda',  # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'need_yhist' : False\n",
    "        }\n",
    "}\n",
    "\n",
    "# Case 2. GRU model (w/o data representation)\n",
    "config2 = {\n",
    "        'model': 'GRU', # Regression에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC', 'DARNN} 중 택 1\n",
    "        'training': True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "        'best_model_path': './ckpt/gru.pt',  # 학습 완료 모델 저장 경로\n",
    "        'with_representation' : False, # representation 유무, bool (defeault: False)\n",
    "        'parameter': {\n",
    "            'input_size': 27,  # 데이터의 변수 개수, int\n",
    "            'timestep' : 10, # timestep = window_size\n",
    "            'shift_size': 1, # shift 정도, int\n",
    "            'num_classes': 1,  # 분류할 class 개수, int\n",
    "            'num_layers': 2,  # recurrent layers의 수, int(default: 2, 범위: 1 이상)\n",
    "            'hidden_size': 64,  # hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "            'dropout': 0.1,  # dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "            'bidirectional': True,  # 모델의 양방향성 여부, bool(default: True)\n",
    "            'num_epochs': 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "            'batch_size': 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "            'lr': 0.0001,  # learning rate, float(default: 0.001, 범위: 0.1 이하)\n",
    "            'device': 'cuda',  # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'need_yhist' : False\n",
    "        }\n",
    "}\n",
    "\n",
    "# Case 3. CNN_1D model (w/o data representation)\n",
    "config3 = {\n",
    "        'model': 'CNN_1D', # Regression에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC', 'DARNN} 중 택 1\n",
    "        'training': True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "        'best_model_path': './ckpt/cnn_1d.pt',  # 학습 완료 모델 저장 경로\n",
    "        'parameter': {\n",
    "            'input_size': 27,  # 데이터의 변수 개수, int\n",
    "            'timestep' : 10, # timestep = window_size\n",
    "            'shift_size': 1, # shift 정도, int\n",
    "            'num_classes': 1,  # 분류할 class 개수, int\n",
    "            'seq_len': 10,  # 데이터의 시간 길이, int\n",
    "            'output_channels': 64, # convolution layer의 output channel, int(default: 64, 범위: 1 이상, 2의 지수로 설정 권장)\n",
    "            'kernel_size': 3, # convolutional layer의 filter 크기, int(default: 3, 범위: 3 이상, 홀수로 설정 권장)\n",
    "            'stride': 1, # convolution layer의 stride 크기, int(default: 1, 범위: 1 이상)\n",
    "            'padding': 0, # padding 크기, int(default: 0, 범위: 0 이상)\n",
    "            'drop_out': 0.1, # dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "            'num_epochs': 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "            'batch_size': 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "            'lr': 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "            'device': 'cuda',  # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'need_yhist' : False\n",
    "        }\n",
    "}\n",
    "\n",
    "# Case 4. DA-RNN model (w/o data representation)\n",
    "config4 = {\n",
    "        'model': 'LSTM_FCNs', # Regression에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC', 'DARNN} 중 택 1\n",
    "        'training': True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "        'best_model_path': './ckpt/lstm_fcn.pt',  # 학습 완료 모델 저장 경로\n",
    "        'parameter': {\n",
    "            'input_size': 27,  # 데이터의 변수 개수, int\n",
    "            'timestep' : 10, # timestep = window_size\n",
    "            'shift_size': 1, # shift 정도, int\n",
    "            'num_classes': 1,  # 분류할 class 개수, int\n",
    "            'num_layers': 1,  # recurrent layers의 수, int(default: 1, 범위: 1 이상)\n",
    "            'lstm_drop_out': 0.4, # LSTM dropout 확률, float(default: 0.4, 범위: 0 이상 1 이하)\n",
    "            'fc_drop_out': 0.1, # FC dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "            'num_epochs': 150, # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "            'batch_size': 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "            'lr': 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "            'device': 'cuda',  # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'need_yhist' : False\n",
    "        }\n",
    "}\n",
    "\n",
    "# Case 5. fully-connected layers (w/ data representation)\n",
    "config5 = {\n",
    "        'model': 'FC', # Regression에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC', 'DARNN} 중 택 1\n",
    "        \"training\": True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "        \"best_model_path\": './ckpt/fc.pt',  # 학습 완료 모델 저장 경로\n",
    "        'parameter': {\n",
    "            'input_size': 27,  # 데이터의 변수 개수(representation 차원), int\n",
    "            'timestep' : 10, # timestep = window_size\n",
    "            'shift_size': 1, # shift 정도, int\n",
    "            'num_classes': 6,  # 분류할 class 개수, int\n",
    "            'drop_out': 0.1, # dropout 확률, float(default: 0.1, 범위: 0 이상 1 이하)\n",
    "            'bias': True, # bias 사용 여부, bool(default: True)\n",
    "            'num_epochs': 150, # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "            'batch_size': 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "            'lr': 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "            'device': 'cuda',  # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'need_yhist' : False\n",
    "        }\n",
    "}\n",
    "\n",
    "# Case 6. DARNN model (w/o data representation)\n",
    "config6 = {\n",
    "        'model': 'DARNN', # Regression에 활용할 알고리즘 정의, {'LSTM', 'GRU', 'CNN_1D', 'LSTM_FCNs', 'FC', 'DARNN} 중 택 1\n",
    "        'training': True,  # 학습 여부, 저장된 학습 완료 모델 존재시 False로 설정\n",
    "        'best_model_path': './ckpt/darnn.pt',  # 학습 완료 모델 저장 경로\n",
    "        'parameter': {\n",
    "            'input_size': 27,  # 데이터의 변수 개수, int\n",
    "            'encoder_hidden_size': 64, # Encoder hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "            'decoder_hidden_size': 64, # Decoder hidden state의 차원, int(default: 64, 범위: 1 이상)\n",
    "            'timestep': 16, # timestep의 크기, int(default: 16, 범위: 1이상),\n",
    "            'shift_size' : 1, # Slicing 시 shift 크기\n",
    "            'encoder_stateful': False, # Encoder의 Stateful 사용여부, bool(default: False)\n",
    "            'decoder_stateful': False, # Decoder의 Stateful 사용여부, bool(default: False)\n",
    "            'num_epochs': 150,  # 학습 epoch 횟수, int(default: 150, 범위: 1 이상)\n",
    "            'batch_size': 64,  # batch 크기, int(default: 64, 범위: 1 이상, 컴퓨터 사양에 적합하게 설정)\n",
    "            'lr': 0.0001,  # learning rate, float(default: 0.0001, 범위: 0.1 이하)\n",
    "            'device': 'cuda',  # 학습 환경, [\"cuda\", \"cpu\"] 중 선택\n",
    "            'need_yhist': True\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cca7e5",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007ceeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13866, 27)\n",
      "(13866,)\n",
      "(5869, 27)\n",
      "(5869,)\n"
     ]
    }
   ],
   "source": [
    "# raw time seires data for regression\n",
    "train = pd.read_csv('./data/train_data.csv')\n",
    "test = pd.read_csv('./data/test_data.csv')\n",
    "\n",
    "train = train.drop('date', axis=1)\n",
    "test = test.drop('date', axis=1)\n",
    "\n",
    "train_x = train.drop('Appliances', axis = 1)\n",
    "train_y = train['Appliances']\n",
    "\n",
    "test_x = test.drop('Appliances', axis = 1)\n",
    "test_y = test['Appliances']\n",
    "\n",
    "train_data = {'x': train_x, 'y': train_y}\n",
    "test_data = {'x': test_x, 'y': test_y}\n",
    "\n",
    "print(train_x.shape)  #shape : (num_of_instance x representation_dims) = (13866, 27)\n",
    "print(train_y.shape) #shape : (num_of_instance) = (13866, )\n",
    "print(test_x.shape)  #shape : (num_of_instance x representation_dims) = (5869, 27)\n",
    "print(test_y.shape)  #shape : (num_of_instance) = (5869, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bceeae",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc494175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model\n",
      "\n",
      "Epoch 1/150\n",
      "train Loss: 21109.8580, R2: -0.9520\n",
      "val Loss: 19662.6211, R2: -1.0342\n",
      "\n",
      "Epoch 10/150\n",
      "train Loss: 17096.3544, R2: -0.5329\n",
      "val Loss: 15943.9041, R2: -0.6785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\Test.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DMQA/project/KUDataRegression/Test.ipynb#ch0000008?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m data_reg\u001b[39m.\u001b[39mbuild_model()  \u001b[39m# 모델 구축\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DMQA/project/KUDataRegression/Test.ipynb#ch0000008?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DMQA/project/KUDataRegression/Test.ipynb#ch0000008?line=6'>7</a>\u001b[0m     best_model \u001b[39m=\u001b[39m data_reg\u001b[39m.\u001b[39;49mtrain_model(model)  \u001b[39m# 모델 학습\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DMQA/project/KUDataRegression/Test.ipynb#ch0000008?line=7'>8</a>\u001b[0m     data_reg\u001b[39m.\u001b[39msave_model(best_model, best_model_path\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mbest_model_path\u001b[39m\u001b[39m\"\u001b[39m])  \u001b[39m# 모델 저장\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DMQA/project/KUDataRegression/Test.ipynb#ch0000008?line=9'>10</a>\u001b[0m pred, mse \u001b[39m=\u001b[39m data_reg\u001b[39m.\u001b[39mpred_data(model, best_model_path\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mbest_model_path\u001b[39m\u001b[39m\"\u001b[39m])  \u001b[39m# 예측\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\main_regression.py:192\u001b[0m, in \u001b[0;36mRegression.train_model\u001b[1;34m(self, init_model)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/main_regression.py?line=187'>188</a>\u001b[0m criterion \u001b[39m=\u001b[39m [nn\u001b[39m.\u001b[39mMSELoss(), R2Loss()]\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/main_regression.py?line=189'>190</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(init_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameter[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/main_regression.py?line=191'>192</a>\u001b[0m best_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain(init_model, dataloaders_dict, criterion, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameter[\u001b[39m'\u001b[39;49m\u001b[39mnum_epochs\u001b[39;49m\u001b[39m'\u001b[39;49m], optimizer)\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/main_regression.py?line=193'>194</a>\u001b[0m \u001b[39mreturn\u001b[39;00m best_model\n",
      "File \u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\models\\train_reg_model.py:129\u001b[0m, in \u001b[0;36mTrain_Test.train\u001b[1;34m(self, model, dataloaders, criterion, num_epochs, optimizer)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=124'>125</a>\u001b[0m \u001b[39m# training 단계에서만 gradient 업데이트 수행\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=125'>126</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=126'>127</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=127'>128</a>\u001b[0m     \u001b[39m# input을 model에 넣어 output을 도출한 역정규화 후, loss를 계산함\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=128'>129</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=129'>130</a>\u001b[0m     outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=130'>131</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion[\u001b[39m0\u001b[39m](outputs, targets)\n",
      "File \u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\models\\rnn.py:36\u001b[0m, in \u001b[0;36mRNN_model.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/rnn.py?line=32'>33</a>\u001b[0m     out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(x, h0)  \u001b[39m# out: tensor of shape (batch_size, seq_length, hidden_size)\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/rnn.py?line=33'>34</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/rnn.py?line=34'>35</a>\u001b[0m     \u001b[39m# initial cell states 설정\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/rnn.py?line=35'>36</a>\u001b[0m     c0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_directions \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, x\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m     <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/rnn.py?line=36'>37</a>\u001b[0m     out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(x, (h0, c0))  \u001b[39m# out: tensor of shape (batch_size, seq_length, hidden_size)\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/rnn.py?line=38'>39</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Case 1. LSTM model (w/o data representation)\n",
    "config = config1\n",
    "data_reg = mr.Regression(config, train_data, test_data)\n",
    "model = data_reg.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_reg.train_model(model)  # 모델 학습\n",
    "    data_reg.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse = data_reg.pred_data(model, best_model_path=config[\"best_model_path\"])  # 예측\n",
    "print(f'test Loss: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2. GRU (w/o data representation)\n",
    "config = config2\n",
    "data_reg = mr.Regression(config, train_data, test_data)\n",
    "model = data_reg.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_reg.train_model(model)  # 모델 학습\n",
    "    data_reg.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse = data_reg.pred_data(model, best_model_path=config[\"best_model_path\"])  # 예측\n",
    "print(f'test Loss: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53858dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 3. CNN_1D (w/o data representation)\n",
    "config = config3\n",
    "data_reg = mr.Regression(config, train_data, test_data)\n",
    "model = data_reg.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_reg.train_model(model)  # 모델 학습\n",
    "    data_reg.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse = data_reg.pred_data(model, best_model_path=config[\"best_model_path\"])  # 예측\n",
    "print(f'test Loss: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 4. LSTM_FCNs (w/o data representation)\n",
    "config = config4\n",
    "data_reg = mr.Regression(config, train_data, test_data)\n",
    "model = data_reg.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_reg.train_model(model)  # 모델 학습\n",
    "    data_reg.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse = data_reg.pred_data(model, best_model_path=config[\"best_model_path\"])  # 예측\n",
    "print(f'test Loss: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e684aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Case 5. fully-connected layers (w/ data representation) -> DARNN 대체\n",
    "# config = config5\n",
    "# data_reg = mr.Regression(config, train_data, test_data)\n",
    "# model = data_reg.build_model()  # 모델 구축\n",
    "\n",
    "# if config[\"training\"]:\n",
    "#     best_model = data_reg.train_model(model)  # 모델 학습\n",
    "#     data_reg.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "# pred, mse = data_reg.pred_data(model, best_model_path=config[\"best_model_path\"])  # 예측\n",
    "# print(f'test Loss: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b679c1e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model\n",
      "\n",
      "Epoch 1/150\n",
      "train Loss: 20839.5268, R2: -0.9284\n",
      "val Loss: 18221.8203, R2: -0.9371\n",
      "\n",
      "Epoch 10/150\n",
      "train Loss: 7161.6474, R2: 0.3974\n",
      "val Loss: 6195.7546, R2: 0.4347\n",
      "\n",
      "Epoch 20/150\n",
      "train Loss: 5067.5503, R2: 0.5541\n",
      "val Loss: 4589.3824, R2: 0.5629\n",
      "\n",
      "Epoch 30/150\n",
      "train Loss: 4627.9111, R2: 0.5806\n",
      "val Loss: 4217.2570, R2: 0.5979\n",
      "\n",
      "Epoch 40/150\n",
      "train Loss: 4520.7092, R2: 0.5837\n",
      "val Loss: 4048.0816, R2: 0.5981\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\Test.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DMQA/project/KUDataRegression/Test.ipynb#ch0000013?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m data_reg\u001b[39m.\u001b[39mbuild_model()  \u001b[39m# 모델 구축\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DMQA/project/KUDataRegression/Test.ipynb#ch0000013?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DMQA/project/KUDataRegression/Test.ipynb#ch0000013?line=6'>7</a>\u001b[0m     best_model \u001b[39m=\u001b[39m data_reg\u001b[39m.\u001b[39;49mtrain_model(model)  \u001b[39m# 모델 학습\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DMQA/project/KUDataRegression/Test.ipynb#ch0000013?line=7'>8</a>\u001b[0m     data_reg\u001b[39m.\u001b[39msave_model(best_model, best_model_path\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mbest_model_path\u001b[39m\u001b[39m\"\u001b[39m])  \u001b[39m# 모델 저장\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DMQA/project/KUDataRegression/Test.ipynb#ch0000013?line=9'>10</a>\u001b[0m pred, mse \u001b[39m=\u001b[39m data_reg\u001b[39m.\u001b[39mpred_data(model, best_model_path\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mbest_model_path\u001b[39m\u001b[39m\"\u001b[39m])  \u001b[39m# 예측\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\main_regression.py:192\u001b[0m, in \u001b[0;36mRegression.train_model\u001b[1;34m(self, init_model)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/main_regression.py?line=187'>188</a>\u001b[0m criterion \u001b[39m=\u001b[39m [nn\u001b[39m.\u001b[39mMSELoss(), R2Loss()]\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/main_regression.py?line=189'>190</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(init_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameter[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/main_regression.py?line=191'>192</a>\u001b[0m best_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain(init_model, dataloaders_dict, criterion, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameter[\u001b[39m'\u001b[39;49m\u001b[39mnum_epochs\u001b[39;49m\u001b[39m'\u001b[39;49m], optimizer)\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/main_regression.py?line=193'>194</a>\u001b[0m \u001b[39mreturn\u001b[39;00m best_model\n",
      "File \u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\models\\train_reg_model.py:101\u001b[0m, in \u001b[0;36mTrain_Test.train\u001b[1;34m(self, model, dataloaders, criterion, num_epochs, optimizer)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=96'>97</a>\u001b[0m \u001b[39m# training 단계에서만 gradient 업데이트 수행\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=97'>98</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=98'>99</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=99'>100</a>\u001b[0m     \u001b[39m# input을 model에 넣어 output을 도출한 역정규화 후, loss를 계산함\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=100'>101</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs, y_hist)\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=101'>102</a>\u001b[0m     outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/train_reg_model.py?line=102'>103</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion[\u001b[39m0\u001b[39m](outputs, targets)\n",
      "File \u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\models\\darnn.py:194\u001b[0m, in \u001b[0;36mDARNN.forward\u001b[1;34m(self, X_history, y_history)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=191'>192</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X_history, y_history): \u001b[39m# X_history : batch_x // y_history : batch_y_h\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=193'>194</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(X_history), y_history) \u001b[39m# (128, 1)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=194'>195</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/my_env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DMQA\\project\\KUDataRegression\\models\\darnn.py:120\u001b[0m, in \u001b[0;36mTemporalAttentionDecoder.forward\u001b[1;34m(self, encoded_inputs, y)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=115'>116</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, encoded_inputs, y): \u001b[39m# encoded_inputs : (128, 16, 64) // y(batch_y_h) : (128, 16, 1)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=116'>117</a>\u001b[0m     \n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=117'>118</a>\u001b[0m     \u001b[39m#initializing hidden states\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=118'>119</a>\u001b[0m     d_tm1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((encoded_inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_hidden_size))\u001b[39m.\u001b[39mcuda() \u001b[39m# out : (128, 64) - decoder hidden state\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=119'>120</a>\u001b[0m     s_prime_tm1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros((encoded_inputs\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder_hidden_size))\u001b[39m.\u001b[39;49mcuda() \u001b[39m# out : (128, 64) - cell state\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=121'>122</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestep\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=122'>123</a>\u001b[0m         \n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=123'>124</a>\u001b[0m         \u001b[39m#concatenate hidden states\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/DMQA/project/KUDataRegression/models/darnn.py?line=124'>125</a>\u001b[0m         d_s_prime_concat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((d_tm1, s_prime_tm1), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# out : (128, 128)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Case 6. DARNN model (w/o data representation)\n",
    "config = config6\n",
    "data_reg = mr.Regression(config, train_data, test_data)\n",
    "model = data_reg.build_model()  # 모델 구축\n",
    "\n",
    "if config[\"training\"]:\n",
    "    best_model = data_reg.train_model(model)  # 모델 학습\n",
    "    data_reg.save_model(best_model, best_model_path=config[\"best_model_path\"])  # 모델 저장\n",
    "\n",
    "pred, mse = data_reg.pred_data(model, best_model_path=config[\"best_model_path\"])  # 예측\n",
    "print(f'test Loss: {mse}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7e6565a69c64523dea2d3b31d6ed9f8445dc9714e18e3f5d2b2bc6eff30a54c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('my_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
